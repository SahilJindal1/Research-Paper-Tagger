{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data.jsonl\") as f:\n",
    "    training_data = json.load(f)\n",
    "with open(\"test_data.jsonl\") as f:\n",
    "    test_data = json.load(f)\n",
    "with open(\"validation_data.jsonl\") as f:\n",
    "    valid_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    train_data.append(training_data[i][0][0] + ' ' + training_data[i][0][2])\n",
    "    \n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "val_data = []\n",
    "\n",
    "for i in range(len(valid_data)):\n",
    "    val_data.append(valid_data[i][0][0] + ' ' + valid_data[i][0][2])\n",
    "    \n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE = re.compile('[/(){}\\[\\]\\|@,;.-_-â€”]')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE.sub(' ', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "train_data = list(map(clean_text, train_data))\n",
    "val_data = list(map(clean_text, val_data))\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "{'generation': 1, 'sentiment analysis, stylistic analysis, and argument mining': 2, 'information extraction, retrieval and text mining': 3, 'resources and evaluation': 4, 'machine learning for nlp': 5, 'interpretability and analysis of models for nlp': 6, 'question answering': 7, 'linguistic theories, cognitive modeling and psycholinguistics': 8, 'discourse and pragmatics': 9, 'machine translation and multilinguality': 10, 'ethics in nlp': 11, 'summarization': 12, 'tagging, chunking, syntax and parsing': 13, 'dialogue and interactive systems': 14, 'semantics': 15, 'computational social science, social media and cultural analytics': 16, 'nlp applications': 17, 'language grounding to vision, robotics and beyond': 18, 'phonology, morphology and word segmentation': 19, 'speech and multimodality': 20}\n"
     ]
    }
   ],
   "source": [
    "diction = {}\n",
    "for i in training_data:\n",
    "    diction[i[1]] = 1\n",
    "    \n",
    "print(len(diction))\n",
    "\n",
    "j = 1\n",
    "for i in diction:\n",
    "    diction[i] = j\n",
    "    j += 1\n",
    "\n",
    "print(diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1386\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "train_labels = []\n",
    "for i in range(len(training_data)):\n",
    "    train_labels.append(diction[training_data[i][1]])\n",
    "    \n",
    "print(len(train_labels))\n",
    "\n",
    "val_labels = []\n",
    "for i in range(len(valid_data)):\n",
    "    val_labels.append(diction[valid_data[i][1]])\n",
    "    \n",
    "print(len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.convert_to_tensor(train_labels, dtype = tf.int64)\n",
    "y_val = tf.convert_to_tensor(val_labels, dtype = tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens 9900\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "tokenizer.fit_on_texts(val_data)\n",
    "print(\"Number of unique tokens\", len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data tensor shape (1386, 185)\n",
      "Validation data tensor shape (175, 185)\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_data)\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(val_data)\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(X_val, maxlen = 185)\n",
    "\n",
    "print('Train data tensor shape', X_train.shape)\n",
    "print('Validation data tensor shape', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 185, 100)          990100    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 185, 100)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 21)                2121      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,072,621\n",
      "Trainable params: 1,072,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length = X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(21, activation = 'softmax'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "22/22 [==============================] - 7s 211ms/step - loss: 2.9720 - accuracy: 0.1068 - val_loss: 2.8255 - val_accuracy: 0.1086\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 4s 175ms/step - loss: 2.8185 - accuracy: 0.1126 - val_loss: 2.8011 - val_accuracy: 0.1771\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 4s 174ms/step - loss: 2.7680 - accuracy: 0.1566 - val_loss: 2.7609 - val_accuracy: 0.1086\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 4s 194ms/step - loss: 2.5996 - accuracy: 0.1818 - val_loss: 2.5970 - val_accuracy: 0.1600\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 4s 203ms/step - loss: 2.3041 - accuracy: 0.2994 - val_loss: 2.5040 - val_accuracy: 0.2114\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 2.0435 - accuracy: 0.4048 - val_loss: 2.6471 - val_accuracy: 0.1943\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 1.8243 - accuracy: 0.5245 - val_loss: 2.4596 - val_accuracy: 0.2114\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 1.6104 - accuracy: 0.5960 - val_loss: 2.3711 - val_accuracy: 0.3086\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 1.3695 - accuracy: 0.6378 - val_loss: 2.3450 - val_accuracy: 0.2914\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 4s 203ms/step - loss: 1.1688 - accuracy: 0.7085 - val_loss: 2.3723 - val_accuracy: 0.3200\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 4s 201ms/step - loss: 1.0769 - accuracy: 0.7460 - val_loss: 2.4581 - val_accuracy: 0.2914\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 4s 201ms/step - loss: 0.8881 - accuracy: 0.7958 - val_loss: 2.5066 - val_accuracy: 0.2743\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 4s 201ms/step - loss: 0.6835 - accuracy: 0.8506 - val_loss: 2.5424 - val_accuracy: 0.3371\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 4s 201ms/step - loss: 0.5477 - accuracy: 0.8817 - val_loss: 2.7548 - val_accuracy: 0.2971\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 5s 206ms/step - loss: 0.4668 - accuracy: 0.8990 - val_loss: 2.6253 - val_accuracy: 0.3429\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 5s 211ms/step - loss: 0.3988 - accuracy: 0.9084 - val_loss: 2.8090 - val_accuracy: 0.3314\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.4682 - accuracy: 0.8867 - val_loss: 2.7436 - val_accuracy: 0.2971\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.3565 - accuracy: 0.9192 - val_loss: 2.8874 - val_accuracy: 0.3143\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.2622 - accuracy: 0.9408 - val_loss: 3.0375 - val_accuracy: 0.3143\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 4s 203ms/step - loss: 0.2231 - accuracy: 0.9618 - val_loss: 3.0368 - val_accuracy: 0.2971\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.1939 - accuracy: 0.9589 - val_loss: 3.0194 - val_accuracy: 0.2914\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.1755 - accuracy: 0.9654 - val_loss: 3.0110 - val_accuracy: 0.3029\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.1637 - accuracy: 0.9668 - val_loss: 3.1891 - val_accuracy: 0.2800\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.1631 - accuracy: 0.9675 - val_loss: 2.9611 - val_accuracy: 0.2743\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.1157 - accuracy: 0.9892 - val_loss: 3.0509 - val_accuracy: 0.3029\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 4s 201ms/step - loss: 0.1040 - accuracy: 0.9834 - val_loss: 2.9341 - val_accuracy: 0.3314\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 4s 201ms/step - loss: 0.0845 - accuracy: 0.9892 - val_loss: 3.3077 - val_accuracy: 0.2914\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 4s 201ms/step - loss: 0.1013 - accuracy: 0.9812 - val_loss: 3.0080 - val_accuracy: 0.3029\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 4s 201ms/step - loss: 0.1081 - accuracy: 0.9784 - val_loss: 2.9626 - val_accuracy: 0.3257\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 0.0648 - accuracy: 0.9935 - val_loss: 3.0660 - val_accuracy: 0.3429\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, validation_data = (X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
